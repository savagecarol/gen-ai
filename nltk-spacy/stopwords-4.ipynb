{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "91b03b18-2aa6-4cba-ba56-aec1be92e141",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = \"\"\"Thank you, David and thank you everyone for inviting me here. This is a huge privilege, not because I’m the chief guest. I think it’s a privilege mainly because I’m one of the parents who have had the opportunity. And I’ll take this opportunity on behalf of all of you to put my hands together and thank Dhirubhai Ambani International School for doing what they’re doing to our children. So, I want to thank all the teachers, all the heads of departments, Zarine and Fareeda, I mean, you’re the people I used to come to, when I have trouble I come and look at your faces and go away, and I’m calm; everything will be sorted out. Kava sir was fantastic at cricket matches and shouts louder than anyone else in the world can, all the staff members, the management, the gentleman who man the security outside; so wonderful and so even the guy who does the parking back there, everyone for the last 13 to 14 years that I have been here. And especially my friend, Mrs. Nita Ambani. Thank you so much for looking after our children. Thank you very very much.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c858bae-3662-4ad8-8665-ec1fcb61afaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7930de2d-1b7c-4b00-b015-d08cdb91c498",
   "metadata": {},
   "outputs": [],
   "source": [
    "##stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7283097d-eee3-4009-9553-41460e6d2866",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/apple/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "from nltk.stem import PorterStemmer , SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4933d703-e2c3-4b10-ba5a-4b7638d94f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "snowballStemmer = SnowballStemmer('english')\n",
    "porterStemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "61beb0c7-c5a2-4d47-9242-64192e9c7c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['thank , david thank everyon invit .', 'this huge privileg , i ’ chief guest .', 'i think ’ privileg main i ’ one parent opportun .', 'and i ’ take opportun behalf put hand togeth thank dhirubhai ambani intern school ’ children .', 'so , i want thank teacher , head depart , zarin fareeda , i mean , ’ peopl i use come , i troubl i come look face go away , i ’ calm ; everyth sort .', 'kava sir fantast cricket match shout louder anyon els world , staff member , manag , gentleman man secur outsid ; wonder even guy park back , everyon last 13 14 year i .', 'and especi friend , mrs. nita ambani .', 'thank much look children .', 'thank much .']\n"
     ]
    }
   ],
   "source": [
    "sentences = sent_tokenize(paragraph)\n",
    "for i in range(len(sentences)):\n",
    "    words = word_tokenize(sentences[i])\n",
    "    value = []\n",
    "    for word in words:\n",
    "        if word not in set(stopwords.words('english')):\n",
    "            value.append(snowballStemmer.stem(word))\n",
    "    sentences[i]  = ' '.join(value)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "78826a8a-7ca1-4988-8eec-8d87813b775b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['thank , david thank everyon invit .', 'thi huge privileg , i ’ chief guest .', 'i think ’ privileg mainli i ’ one parent opportun .', 'and i ’ take opportun behalf put hand togeth thank dhirubhai ambani intern school ’ children .', 'so , i want thank teacher , head depart , zarin fareeda , i mean , ’ peopl i use come , i troubl i come look face go away , i ’ calm ; everyth sort .', 'kava sir fantast cricket match shout louder anyon els world , staff member , manag , gentleman man secur outsid ; wonder even guy park back , everyon last 13 14 year i .', 'and especi friend , mrs. nita ambani .', 'thank much look children .', 'thank much .']\n"
     ]
    }
   ],
   "source": [
    "sentence2 = sent_tokenize(paragraph)\n",
    "for i in range(len(sentence2)):\n",
    "    words = word_tokenize(sentence2[i])\n",
    "    value = []\n",
    "    for word in words:\n",
    "        if word not in set(stopwords.words('english')):\n",
    "            value.append(porterStemmer.stem(word))\n",
    "    sentences[i]  = ' '.join(value)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c0d2e1fb-3deb-496e-9e84-f148b05ece3c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'WordNetLemmatizer' object has no attribute 'lematize'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[77], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words:\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mset\u001b[39m(stopwords\u001b[38;5;241m.\u001b[39mwords(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m)):\n\u001b[0;32m----> 7\u001b[0m             value\u001b[38;5;241m.\u001b[39mappend(\u001b[43mlemmatizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlematize\u001b[49m(word))\n\u001b[1;32m      8\u001b[0m     sentences[i]  \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(value)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(sentences)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'WordNetLemmatizer' object has no attribute 'lematize'"
     ]
    }
   ],
   "source": [
    "sentence3 = sent_tokenize(paragraph)\n",
    "for i in range(len(sentence2)):\n",
    "    words = word_tokenize(sentence2[i])\n",
    "    value = []\n",
    "    for word in words:\n",
    "        if word not in set(stopwords.words('english')):\n",
    "            value.append(lemmatizer.lematize(word))\n",
    "    sentences[i]  = ' '.join(value)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2e5a5f-af34-493c-ac4c-f8855f0069c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
